生成对抗网络的基础知识：

>   Ref: https://neptune.ai/blog/gan-loss-functions

#### 原始的GAN loss

GAN的核心idea就是两个网络通过min-max博弈，找到一个纳什均衡点，使得生成器G可以完美骗过判别器D。因此，原始的GAN loss就是一个min max function：
$$
GANloss = min_G max_D \{E_{x\sim p_{data}(x)}[\log(D(x))] + E_{z\sim p(z) }[\log(1-D(G(z)))] \}
$$
进一步理解：

当G固定时，可以算出D的最优解：
$$
D^* = \frac{p_{data}(x)}{p_{data}(x)+p_{g}(x)}
$$
此时，生成器的损失函数为：
$$
L(G) = 2JS(p_{data}(x)||p_g(x)) - 2\log 2
$$
**JS散度（Jensen-Shannon散度）：**
$$
JS(p_1||p_2) = \frac{1}{2}KL(p_1||\frac{1}{2}(p_1 + p_2)) + \frac{1}{2}KL(p_2||\frac{1}{2}(p_1 + p_2))
$$
可以看出，JS散度和KL散度类似，是度量分布的差异性的指标，而且，JS对KL做了对称化，类似于PSI指数（PSI指数是对KL(p1||p2)和KL(p2||p1)直接求均值）。

这里可以看出，当D确定时，G的目标就是在优化生成的分布于数据真实分布之间的JS散度。**JS散度只在两个分布有重叠的区域才有梯度**（证明见下面）。因此，如果两个分布差的很远，就没法进行优化更新了。这是非常致命的一个缺点。

证明：JS散度只能在两个分布有重叠时才有梯度，否则为常数。

在分布不重叠的情况下，没有p1和p2都不为零的区域，因此，只需要考虑三个部分：

p1=0，p2不等于0：这个p1=0直接使得左边的一项KL为0，而右边则变成了KL(p2||p2/2)，为常数。

反之，p1不等于0，p2等于0：对称性，右边为0，左边常数。

总之，两个分布不重叠时，梯度消失。

而在高维空间中，原始数据分布为流形，此时两者分布有交集的概率很大，即P(measure=0)=1。

这是GAN的一个严重的问题，后面的改进也是基于这个问题。



#### GAN 存在的问题

-   难以判断何时收敛。
-   模式坍塌（mode collapse）。即没有学到多mode中的流形，而是进入了单一mode或者几个mode的局部最优。（ref: https://aiden.nibali.org/blog/2017-01-18-mode-collapse-gans/）
-   效果评价标准不好确定。



#### 视觉任务中的GAN的特点

原始的GAN输入是noise，但是实际的任务中，监督数据一般是给定的（输入，输出）对。因此需要一些策略保证生成式模型的优点，又能利用真实label。通常直接将输入图像输入进G中，然后用dropout来模拟noise作为生成式模型的扰动。

DCGAN通过noise生成隐向量，然后通过deconv上采样得到大图。但是无法控制输出，只能保证属于分布。

low-level中常用GAN进行超分、去噪、inpainting等任务。由于大量图像训练可以带来分布的先验，因此用GAN拟合这个分布是合理的。

常见的image to image的模型：SRGAN、ESRGAN、PAN、pix2pix等。

