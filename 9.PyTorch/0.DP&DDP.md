DP和DDP是pytorch中常用的两种并行化策略。



- DP只能用于**单机多卡**；DDP适合于**多机多卡（当然也可以单机多卡）**。
- DP是基于**parameter server**并行策略的，而DDP基于**allreduce**并行。
- DP相对于DDP效率更低（**GIL锁**，由于单进程多线程）
- **DP是多线程，DDP多进程**。因此DDP需要用pytorch的launch来运行DDP程序。



#### pytorch中的DP（data parallel）机制

数据并行的思路就是讲一个batch拆分成多个等大小的batch，分配给不同的GPU进行运算，最后将结果进行整合，得到最终的输出。

DataParallel适用于单机多卡的场景。

pytorch对应的api：

> https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel

~~~python
torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)
参数
module: 网络模型
device_ids: 需要并行的gpu编号，list形式，如[0,1,2]
output_device: 输出结果存放的位置，一般是cuda:0，第一张卡
~~~

实现方式简单，只需要用DataParallel将原model进行wrap即可。其余代码无需改动。

~~~python
model = nn.DataParallel(model)
~~~

DP的基本原理是：以一张卡作为reducer，首先将模型分发给各个GPU，将一个batch的数据进行拆分，分给各个GPU各自运算，然后再reducer上进行融合处理。这个操作会导致负载不均衡，reducer卡开销比其他卡要更大。

需要注意：被wrap的model在每次forward操作时，都要重新将cuda0（reducer卡）上的module参数copy到各个GPU，因此，各个GPU上运行的实际上是model的副本，并且这些副本在一次传播结束之后就被销毁。所以，在forward中更新全局变量在DP模式下是不可取的。

forward之后，将得到的结果汇集到cuda0进行梯度计算，更新参数。准备下次再次分发模型到各个GPU（parameter server模式）。

DP不支持apex半精度训练。



#### DDP（distributed data parallel）并行化

DDP本意是针对多机多卡的情况，即多个node组成的GPU计算集群，进行分布式训练的。但是DDP也可以用于单机多卡，并且比DP效率更高。因为DDP是基于多进程（MultiProcessing），一般是一个进程控制一个GPU进行计算。



**DDP会用到一些环境变量：**

- RANK：所有机器上所有进程中的序号。

- LOCAL_RANK：该node上的进程编号，一般设定为GPU号

- WORLD_SIZE：总进程数，即num of node x num of gpus
- MASTER_ADDR：主节点的地址（ip地址或者主机名称，单机可以localhost或者127.0.0.1）
- MASTER_PORT：主节点监听端口，注意，各个机器上的process都要指定同样的addr和port以便找到主节点。
- CUDA_VISIBLE_DEVICES：本机器可用的卡，默认都用，如果指定，则rank类的编号只从这些选择的GPU上进行编号。比如CUDA_VISIBLE_DEVICES=[2,3]，则gpu2为local_rank 0。

比如，用2台机器，每台机器4张GPU训练，那么rank就是[0,1,2,3,4,5,6,7]，第一台机器上的local_rank为[0,1,2,3]，第二台机器上的local_rank为[4,5,6,7]。



**DDP的基本原理：**

DDP采用ring-allreduce模式执行分布式训练，不同于DP中将reduce放在单一GPU上（从而会有执行快的等待执行慢的从而降低效率的问题），DDP通过各个进程对于前后两个进程的通信传递数据，多次迭代后即可完成数据同步。



**DDP的代码修改：**

将一个普通的GPU训练代码改写成DDP的方法如下：

- 程序开始前增加一句多进程组初始化：

~~~python
torch.distributed.init_process_group(backend="nccl")
~~~

该函数可以设置所用的backend，以及显式指定具体的world size rank等进程组中所用的参数。函数的签名为：

~~~python
def init_process_group(backend,
                       init_method=None,
                       timeout=default_pg_timeout,
                       world_size=-1,
                       rank=-1,
                       store=None,
                       group_name='',
                       pg_options=None):
~~~

- 指定当前进程所用的gpu

```python
torch.cuda.set_device(local_rank) # 这个语句指定所用的gpu，但是如果后面显式model.to("cuda:0")之类的语句，那么该句就被覆盖。不建议使用。
device = torch.device("cuda", local_rank)
```

这里的local_rank参数可以通过os获取环境变量得到，如果用torch.distributed.launch的话，会给脚本增加一个local_rank的参数，可以在训练的main函数中通过args获取。

- 用DDP包装model：

~~~python
model = DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)
~~~

注意，这里的DDP由于一个进程只占用一块GPU，多GPU是通过多进程实现的，因此device_ids只需要[local_rank]即可。

在用DDP包装模型时，如果需要加载已有模型，一般先将模型放在cpu，再通过to(device)移动到指定的GPU。这是因为rank=0保存的模型可能存储了模型的location，导致都所有process的模型都加载到rank=0的卡上。

另外，DDP不能自动对数据集进行拆分，因此需要对Dataloader指定用DistributedSampler。然后后续步骤就和普通的单机单卡一样训练即可。

如果有print或者log的情况，可以通过local_rank进行判断，只让rank=0的进程打印日志，防止多次重复打印。另外，模型保存一般也只用rank=0进行保存。

还需注意：在开始多进程时，需要将各个进程的初始模型同步，可以通过一个主进程生成一个初始化模型，存放在临时位置，其他进程去取，保证初始模型一致。



**DDP的启动方式**

- 将上面提到的必要的环境变量指定好（如MASTER_PORT、LOCAL_RANK之类），然后分别正常启动。
- 利用torch.distributed.launch命令启动。如下：

~~~shell
export MASTER_PORT=XXX
export MASTER_ADDR=XXX
PYTHON=/path/to/your/python
nohup $PYTHON -m torch.distributed.launch \
        --nnodes=1 \
        --nproc_per_node=4 \
        --node_rank=0 \
        --master_addr=$MASTER_ADDR \
        --master_port=$MASTER_PORT \
        train.py [--(train script args)]
~~~

其中，nnodes指的是一共几个节点（机器），nproc_per_node就是每个机器起几个进程（一般为GPU个数），后面是master节点的地址和端口。



