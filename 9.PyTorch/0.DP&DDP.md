DP和DDP是pytorch中常用的两种并行化策略。

- DP适合与单机多卡；DDP适合于多机多卡。
- DP是基于parameter server并行策略的，而DDP基于allreduce并行。
- DP相对于DDP效率更低（GIL锁）
- DP是多线程，DDP多进程。因此DDP需要用pytorch的launch来运行DDP程序。
- 



#### pytorch中的DP（data parallel）机制

数据并行的思路就是讲一个batch拆分成多个等大小的batch，分配给不同的GPU进行运算，最后将结果进行整合，得到最终的输出。

DataParallel适用于单机多卡的场景。

pytorch对应的api：

> https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel

`torch.nn.DataParallel(*module*, *device_ids=None*, *output_device=None*, *dim=0*)`

实现方式简单，只需要用DataParallel将原model进行wrap即可。其余代码无需改动。

~~~python
model = nn.DataParallel(model)
~~~

DP的基本原理是：以一张卡作为reducer，首先将模型分发给各个GPU，将一个batch的数据进行拆分，分给各个GPU各自运算，然后再reducer上进行融合处理。这个操作会导致负载不均衡，reducer卡开销比其他卡要更大。

