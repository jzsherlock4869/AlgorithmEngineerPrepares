最基础的几种机器学习模型，整理主要的知识点。



#### SVM (support vector machine)



**核方法的原理？**

利用二元函数取代实际的升维+两两相乘的计算，对于较难显式升维的情况（如gauss核函数对应的无穷维），可以不用升维而得到升维加内积一样的效果。

核方法在很多场景都有用。【待补充】



**KKT条件与对偶：**

KKT 条件：

- 原问题约束
- 对偶问题约束
- 互补松弛（complementary slackness）
- 稳定性（导数为0，stability）

互补松弛决定了support vector的性质：只有约束有效的样本，lambda才大于0，从而才对模型的结果有贡献。其他的样本无贡献，删去无影响。



**为何采用对偶？**

拉格朗日乘子法（等式约束）的推广，消去复杂约束条件，方便计算。【待补充：convex opt相关理论，slater条件，KKT条件等】



#### LR (logistic regression)

LR模型的特点？本质是一个线性模型＋sigmoid。

优点：简单，鲁棒，可解释性好。常用在对特征变量需要严格的解释性并且需严格控制风险的场景（比如银行借贷）。

**LR的多重共线性问题？**

当多个特征线性相关时，会导致LR的系数w不稳定。因为这些特征可以相互线性表出，从而对应改变各自的系数（符合约束的条件下）几乎不会影响训练结果。但是这些系数可能不正常（如abs较大），并不能代表真实的权重。多重共线性应该在特征工程的时候就被发现并处理掉。

**LR模型对于连续特征的处理？**

由于LR本质是一个线性模型（仅在最后做一次非线性变换），因此，在处理特征的时候，只适用于**单调连续特征**。当特征与label不再单调时（比如：以消费水平作为目标，年龄这个特征就是非单调的，年龄过小或过大消费水平都降低，中年人往往消费水平高），LR就无法比较好的拟合。

考虑其原因，根本在于LR对于一个feature只有一个weight，正数表示正向，负数表示负向，所以导致只有两个方向，即单调增或单调减。

解决方法：引入非线性，最常用的就是**分箱后做one-hot**。这样可以将一个特征转化为多个。仍以上面例子，将年龄分段，得到：小孩、年轻人、中年人、老人 这几个特征，那么可以分别进行加权，中年人weight为大的正数，小孩老人为负数。



#### 决策树模型 (decision tree)

ID3/C4.5/CART树的关键点：（分裂原则，分类回归，categorical/numerical）

| 模型 | 分裂原则                    | 分类？回归？ | 类别变量？连续变量？ | 其他   | 其他                           |
| ---- | --------------------------- | ------------ | -------------------- | ------ | ------------------------------ |
| ID3  | 信息增益                    | 分类         | 类别                 | 无     |                                |
| C4.5 | 信息增益比                  | 分类+回归    | 类别 or 连续         | 预剪枝 |                                |
| CART | 基尼系数（分类）MSE（回归） | 分类+回归    | 类别 or 连续         | 后剪枝 | GBDT的基分类器，可以处理缺失值 |

**为什么树模型不适合one-hot编码？**

one-hot编码的类别特征，特别是高维稀疏特征，即种类很多的，会在one-hot后的特征维度上出现只有很少的1，大部分都是0的情况（f=a，少量，取值1;  f != a，剩下所有，取值0）。从而分裂该特征后，某个节点上样本数很少，统计意义不明显，另外**增益由于需要乘以样本数比例而变小**，分裂的效益也不明显。




