最基础的几种机器学习模型，整理主要的知识点。



#### SVM (support vector machine)

**核方法的原理**：利用二元函数取代实际的升维+两两相乘的计算，对于较难显式升维的情况（如gauss核函数对应的无穷维），可以不用升维而得到升维加内积一样的效果。

核方法在很多场景都有用。【待补充】



KKT条件与对偶：

KKT 条件：

- 原问题约束
- 对偶问题约束
- 互补松弛（complementary slackness）
- 稳定性（导数为0，stability）

互补松弛决定了support vector的性质：只有约束有效的样本，lambda才大于0，从而才对模型的结果有贡献。其他的样本无贡献，删去无影响。



为何采用对偶？

拉格朗日乘子法（等式约束）的推广，消去复杂约束条件，方便计算。【待补充：convex opt相关理论，slater条件，KKT条件等】



#### LR (logistic regression)

LR模型的特点？本质是一个线性模型＋sigmoid。

**LR的多重共线性问题？**

当多个特征线性相关时，会导致LR的系数w不稳定。因为这些特征可以相互线性表出，从而对应改变各自的系数（符合约束的条件下）几乎不会影响训练结果。但是这些系数可能不正常（如abs较大），并不能代表真实的权重。多重共线性应该在特征工程的时候就被发现并处理掉。





#### 决策树模型 (decision tree)

ID3/C4.5/CART树的关键点：（分裂原则，分类回归，categorical/numerical）

| 模型 | 分裂原则                    | 分类？回归？ | 类别变量？连续变量？ | 其他   | 其他                           |
| ---- | --------------------------- | ------------ | -------------------- | ------ | ------------------------------ |
| ID3  | 信息增益                    | 分类         | 类别                 | 无     |                                |
| C4.5 | 信息增益比                  | 分类+回归    | 类别 or 连续         | 预剪枝 |                                |
| CART | 基尼系数（分类）MSE（回归） | 分类+回归    | 类别 or 连续         | 后剪枝 | GBDT的基分类器，可以处理缺失值 |







