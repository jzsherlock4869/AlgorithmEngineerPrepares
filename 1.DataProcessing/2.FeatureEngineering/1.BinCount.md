特征分箱的基本原则与操作方式梳理



#### 动机（为什么要对连续特征进行分箱（离散化）？）

- **非线性**：对于LR（线性模型），增加非线性，用于处理非单调变量。
- **鲁棒性（稳定性）**：提高模型鲁棒性，减少过拟合风险，降低outilier的影响。
- **效率**：稀疏存储与矩阵计算；便于增加或删除某些特征而不影响其他特征。
- **特征交叉**：有利于进一步做特征交叉与关联，应用高阶特征。
- **数据尺度**：可以将所有特征归到同样的尺度（scale）上，防止各特征数值大小的可能影响。



#### 原则（分箱时应该按照那些规则进行？）

- **有序**：年龄、学历、收入等可以保持分箱后的有序性。
- **单调**：尽量保证分箱后保持对于待学习的label具有单调性。
- **平均**：尽量保证分箱后的每个箱中的数量充足（箱数不能太多）并且平均（适当合并一些差别不大的组）。



#### 无监督分箱方法

**等频分箱**

根据特征的分布，使得分箱后的每箱的样本数量尽量相等。

等频分箱可以用pandas中的`pandas.qcut`函数实现：

> pandas.qcut(*x*, *q*, *labels=None*, *retbins=False*, *precision=3*, *duplicates='raise'*)[[source\]](https://github.com/pandas-dev/pandas/blob/v1.2.5/pandas/core/reshape/tile.py#L288-L368)[¶](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html#pandas.qcut)



**等距分箱**

等距分箱的直接根据连续特征的取值范围，等差进行划分，使得所有箱最大最小值的差尽量相等。

pandas中等距分箱可以用函数`pandas.cut`实现：

> pandas.cut(*x*, *bins*, *right=True*, *labels=None*, *retbins=False*, *precision=3*, *include_lowest=False*, *duplicates='raise'*, *ordered=True*)[[source\]](https://github.com/pandas-dev/pandas/blob/v1.2.5/pandas/core/reshape/tile.py#L32-L285)



#### 有监督分箱方法

**卡方分箱**

卡方分箱（ChiMerge）的基本原理是：

~~~
首先将每个特征值视为一个bin，统计频数，然后进行排序。
对于排序后的当前的bins，依次进行扫描并计算卡方值，找到卡方值最小的两个组。
    如果最小卡方值小于阈值，或者bins的数量还大于设定的组数，则进行合并，否则跳出循环。
~~~

卡方分箱的合并过程本质上是一种自下而上的聚类，它的度量准则是卡方值。卡方值是由统计学中的假设检验理论得出的，它度量的是观察值和预期值的差距。因此，卡方越小，说明观察和预测的分布就越接近，反之，则说明差距越大。

对于卡方分箱的过程，卡方的计算是对于两个相邻的（排序后）bin进行计算的。首先，两个相邻的bin应该是如下的形式：

| bin  | pos  | neg  | 总计 |
| ---- | ---- | ---- | ---- |
| 1    | P1   | N1   | A1   |
| 2    | P2   | N2   | A2   |
| 总计 | P    | N    | T    |

这四个格子，每个格子都能计算一个卡方值。如果两个bin合并，那么pos/neg的比例为P/N。而对于bin1和bin2来说，各自的pos/neg比例为P1/N1和P2/N2。同理，neg/pos的比例也一样。卡方的目的就是用合并后的分布去检验原来的两个bin的正负样本的分布，误差会有多大。最后，将所有格子的误差加起来，就是这两个bin合并的代价。

具体计算，以bin1的pos格子为例：

计算期望值：$E_{11} = A_1 * P / T$

而实际值为：$P1$

计算卡方：$\chi^2 = \frac{(E_{11} - P1)^2}{E_{11}}$

其他格子计算方法同理。



**bestKS分箱**

best KS 分箱，顾名思义，即找到特征KS值最大的点进行划分，然后对划分后的bins进行递归找best KS。

KS在模型评估中用于检验模型评分对于正负样本的区分度。其实，按照模型的评分排序并计算KS的过程，可以直接推广到特征的KS上。或者反过来说，模型的评分一旦确定， 也可以看做一个强特征，而模型KS计算的就是这个特征的KS。

特征KS的计算方式：按照特征取值对样本进行排序，然后依次从top到bottom增加选择的样本，然后计算这些已经选中的样本中正样本占总正样本的比例pos_ratio，负样本占总负样本的比例neg_ratio。当全部选中时，pos_ratio=neg_ratio=0。在每个点都能计算出pos_ratio - neg_ratio的值，即当前阈值（或者取值数目）下的KS。best KS即所有KS中最大值。

