ELMo（Embeddings from Language Models）是一种上下文相关的词嵌入方法，可以处理一词多义。

ELMo模型的关键特性：

- **结构为多层bi-LSTM，输入静态词向量，可以编码不同层级的信息**（越向下词义本身的信息越多，越向上上下文的影响越大）。

- **动态嵌入。**预训练完成后，在下游子任务应用时，需要先将句子经过预训练模型，从而得到的编码是与上下文有关的。同一个词由于在不同句子中，编码出的词向量也可以不同，这样就处理了一词多意的问题（比如bank如果和loan、money在一个句子里，那就编码成银行的意义的向量，如果和river、tree在一起，就编码成河岸意义的向量。）
- **下游子任务微调**。多层输出的结果需要加权融合，权重可以在下游子任务中调整。



> ref：https://arxiv.org/pdf/1802.05365.pdf



ELMo的工作原理和结构如图：



![img](assets/640.png)

可以看出，elmo的基本结构就是stacked bi-LSTM。最终用到的实际上是各个输出层的embedding的加权和。

训练elmo的目标函数：最大化似然

![image-20210629162107121](assets/image-20210629162107121.png)



对于下游任务，通过学习得到各级embedding的权重，根据任务进行融合。

![image-20210629162147415](assets/image-20210629162147415.png)

