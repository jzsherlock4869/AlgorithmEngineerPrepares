对于梯度爆炸问题，一个解决方案就是gradient clipping或者gradient norm。首先，设定一个阈值thr，对于超过thr的梯度，进行scale或者直接clip到thr，由此可以避免梯度过大引起的不收敛。

具体实现步骤：

首先，设置阈值thr，对于每次迭代，计算梯度g；

然后，计算g的范数，并与thr进行比较。如果|g|>thr，则计算ratio = thr/|g| ，ratio为缩放系数。

将g乘以ratio，得到norm clip过的结果。此时g的范数就是thr。



注意，gradient clip不同于weight clipping。weight clipping本质上是一种对w的正则；而gradient clipping是对于BP的计算中避免梯度爆炸的手段。



Pytorch中对于gradient clip的实现：

```python
optimizer.zero_grad()        
loss, hidden = model(data, hidden, targets)
loss.backward()

torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)
optimizer.step()
```

其中``` clip_grad_norm_```即对gradient进行裁切，函数实现的核心代码如下：

```python
    clip_coef = max_norm / (total_norm + 1e-6)
    if clip_coef < 1:
        for p in parameters:
            p.grad.detach().mul_(clip_coef.to(p.grad.device))
    return total_norm
```

