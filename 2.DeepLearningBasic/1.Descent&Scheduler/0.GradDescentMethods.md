常见的梯度下降简单总结，从SGD到AdamW

整体思路在于对于不同的维度施加不同的步长，保留前面的下降历史结果作为设置步长的参考，从而使得各个方向上下降更稳定，减少不同方向尺度绝对值大小的影响。





GD/SGD/mini-batch SGD





Momentum





Nesterov Gradient





RMSprop





Adagrad





Adam/AdamW















